{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from itertools import repeat\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "import keras.callbacks as cb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "new_width=64\n",
    "new_height=128\n",
    "channel=3\n",
    "epochCount=12\n",
    "miniBatchSize=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    model_json = model.to_json()\n",
    "    with open(\"modelD2.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"modelD2.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pos images from path specified at posList\n",
    "def loadPosImage(lst):\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        width, height = im.size   # Get dimensions\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "        im=im.crop((left, top, right, bottom))\n",
    "        im=np.asarray(im)\n",
    "        _,_,z=im.shape\n",
    "        if(z==4):\n",
    "            im=np.delete(im,3,axis=2)\n",
    "        #im=im.reshape(new_width*new_height*3)\n",
    "        result.append(im)\n",
    "    return result\n",
    "\n",
    "def loadNegImage(lst):\n",
    "    scaleRatio=1.5 #tuned with training dataset\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        baseWidth=im.size[0]\n",
    "        baseHeight=im.size[1]\n",
    "        im = im.resize((int(baseWidth/scaleRatio),int(baseHeight/scaleRatio)), Image.ANTIALIAS)\n",
    "        width, height = im.size   # Get dimensions\n",
    "        width=width-new_width\n",
    "        height=height-new_height\n",
    "        #select top left from available range\n",
    "        curWidth=0\n",
    "        curHeight=0\n",
    "        while(curWidth<width):\n",
    "            curHeight=0\n",
    "            while(curHeight<height):\n",
    "                #print(curWidth, curHeight)\n",
    "                top=curHeight\n",
    "                left=curWidth\n",
    "                img=im.crop((left, top, left+new_width, top+new_height))\n",
    "                img=np.asarray(img)\n",
    "                _,_,z=img.shape\n",
    "                if(z==4):\n",
    "                    img=np.delete(img,3,axis=2)\n",
    "                #im=im.reshape(new_width*new_height*3)\n",
    "                result.append(img)\n",
    "                #move height 128 pixel a time\n",
    "                curHeight+=128\n",
    "            curWidth+=64\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDShuffle(inputList, size,sd):\n",
    "    np.random.seed(sd)\n",
    "    np.random.shuffle(inputList);\n",
    "    length=len(inputList)\n",
    "    batchNum=int(np.ceil(length/size))\n",
    "    newList=[]\n",
    "    for i in range(batchNum-2):\n",
    "        newList.append(inputList[i*size:(i+1)*size])\n",
    "    newList.append(inputList[(batchNum-1)*size:])\n",
    "    return np.array(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg images are stored at: ./INRIAPerson/train_64x128_H96/neg.lst\n",
      "pos images are stored at: ./INRIAPerson/train_64x128_H96/pos.lst\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"./INRIAPerson/train_64x128_H96\"\n",
    "neg=\"neg.lst\"\n",
    "pos='pos.lst'\n",
    "neg=os.path.join(train_dir, neg)\n",
    "pos=os.path.join(train_dir, pos)\n",
    "print(\"neg images are stored at:\",neg)\n",
    "print(\"pos images are stored at:\",pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two lists that contain locations of positive and negative images\n",
    "posList=[]\n",
    "negList=[]\n",
    "with open(neg, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        negList.append(line)\n",
    "with open(pos, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        posList.append(line)\n",
    "#Repeat each item in negList 3 times, for image reuse\n",
    "#negList = [x for item in negList for x in repeat(item, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416  pos images,  8156 neg images\n"
     ]
    }
   ],
   "source": [
    "posImgList=loadPosImage(posList)\n",
    "negImgList=loadNegImage(negList)\n",
    "posLength=len(posImgList)\n",
    "negLength=len(negImgList)\n",
    "print(posLength, \" pos images, \", negLength, \"neg images\")\n",
    "#create tag for each image\n",
    "posTag=[[0,1]]*posLength\n",
    "negTag=[[1,0]]*negLength\n",
    "dataList=[]\n",
    "dataList.extend(posImgList)\n",
    "dataList.extend(negImgList)\n",
    "dataList=np.array(dataList)\n",
    "#dataList=np.append(posImgList, negImgList, axis = 0)\n",
    "dataTag=[]\n",
    "dataTag.extend(posTag)\n",
    "dataTag.extend(negTag)\n",
    "dataTag=np.array(dataTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(5, 5),\n",
    "                activation='relu',\n",
    "                input_shape=(new_height, new_width, 3)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 2s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "332/332 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "332/332 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 2s 4ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 2s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n",
      "512/512 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-60c4c60415e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         model.fit(mini_images[i], mini_labels[i], batch_size=len(mini_images[i][0]),\n\u001b[0;32m---> 11\u001b[0;31m                 callbacks=[history],validation_data=(test_images[i], test_labels[i]), verbose=0)\n\u001b[0m\u001b[1;32m     12\u001b[0m         score = model.evaluate(test_images[i], test_labels[i], batch_size=miniBatchSize\n\u001b[1;32m     13\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "for j in range(epochCount):\n",
    "    sd=np.random.randint(0,1000)\n",
    "    mini_images=SGDShuffle(dataList,miniBatchSize,sd)\n",
    "    mini_labels=SGDShuffle(dataTag,miniBatchSize,sd)\n",
    "    sd=np.random.randint(0,1000)\n",
    "    test_images=SGDShuffle(dataList,miniBatchSize,sd)\n",
    "    test_labels=SGDShuffle(dataTag,miniBatchSize,sd)\n",
    "    for i in range(len(mini_labels)):\n",
    "        model.fit(mini_images[i], mini_labels[i], batch_size=len(mini_images[i][0]),\n",
    "                callbacks=[history],validation_data=(test_images[i], test_labels[i]), verbose=0)\n",
    "        score = model.evaluate(test_images[i], test_labels[i], batch_size=miniBatchSize\n",
    "        )\n",
    "print(score)\n",
    "plt.plot(history.losses)\n",
    "plt.title('Loss per batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
