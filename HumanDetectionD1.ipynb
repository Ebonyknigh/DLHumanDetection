{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from itertools import repeat\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "import keras.callbacks as cb\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "new_width=64\n",
    "new_height=128\n",
    "channel=3\n",
    "epochCount=10\n",
    "miniBatchSize=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(name):\n",
    "    model_json = model.to_json()\n",
    "    jsName=name+'.json'\n",
    "    h5Name=name+'.h5'\n",
    "    with open(jsName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(h5Name)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg images are stored at: ./INRIAPerson/train_64x128_H96/neg.lst\n",
      "pos images are stored at: ./INRIAPerson/train_64x128_H96/pos.lst\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"./INRIAPerson/train_64x128_H96\"\n",
    "neg=\"neg.lst\"\n",
    "pos='pos.lst'\n",
    "neg=os.path.join(train_dir, neg)\n",
    "pos=os.path.join(train_dir, pos)\n",
    "print(\"neg images are stored at:\",neg)\n",
    "print(\"pos images are stored at:\",pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pos images from path specified at posList\n",
    "def loadPosImage(lst):\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        width, height = im.size   # Get dimensions\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "        im=im.crop((left, top, right, bottom))\n",
    "        im=np.asarray(im)\n",
    "        _,_,z=im.shape\n",
    "        if(z==4):\n",
    "            im=np.delete(im,3,axis=2)\n",
    "        #im=im.reshape(new_width*new_height*3)\n",
    "        result.append(im)\n",
    "    return result\n",
    "\n",
    "def loadNegImage(lst):\n",
    "    scaleRatio=1.5 #tuned with training dataset\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        baseWidth=im.size[0]\n",
    "        baseHeight=im.size[1]\n",
    "        im = im.resize((int(baseWidth/scaleRatio),int(baseHeight/scaleRatio)), Image.ANTIALIAS)\n",
    "        width, height = im.size   # Get dimensions\n",
    "        width=width-new_width\n",
    "        height=height-new_height\n",
    "        #select top left from available range\n",
    "        curWidth=0\n",
    "        curHeight=0\n",
    "        while(curWidth<width):\n",
    "            curHeight=0\n",
    "            while(curHeight<height):\n",
    "                #print(curWidth, curHeight)\n",
    "                top=curHeight\n",
    "                left=curWidth\n",
    "                img=im.crop((left, top, left+new_width, top+new_height))\n",
    "                img=np.asarray(img)\n",
    "                _,_,z=img.shape\n",
    "                if(z==4):\n",
    "                    img=np.delete(img,3,axis=2)\n",
    "                #im=im.reshape(new_width*new_height*3)\n",
    "                result.append(img)\n",
    "                #move height 32 pixel a time\n",
    "                curHeight+=128\n",
    "            curWidth+=64\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two lists that contain locations of positive and negative images\n",
    "posList=[]\n",
    "negList=[]\n",
    "with open(neg, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        negList.append(line)\n",
    "with open(pos, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        posList.append(line)\n",
    "#Repeat each item in negList 3 times, for image reuse\n",
    "#negList = [x for item in negList for x in repeat(item, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416  pos images,  8156 neg images\n"
     ]
    }
   ],
   "source": [
    "posImgList=loadPosImage(posList)\n",
    "negImgList=loadNegImage(negList)\n",
    "posLength=len(posImgList)\n",
    "negLength=len(negImgList)\n",
    "print(posLength, \" pos images, \", negLength, \"neg images\")\n",
    "#create tag for each image\n",
    "posTag=[[0,1]]*posLength\n",
    "negTag=[[1,0]]*negLength\n",
    "dataList=[]\n",
    "dataList.extend(posImgList)\n",
    "dataList.extend(negImgList)\n",
    "#Input feature Scaling\n",
    "dataList=np.array(dataList)/255\n",
    "#dataList=np.append(posImgList, negImgList, axis = 0)\n",
    "dataTag=[]\n",
    "dataTag.extend(posTag)\n",
    "dataTag.extend(negTag)\n",
    "dataTag=np.array(dataTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDShuffle(inputList, size,sd):\n",
    "    np.random.seed(sd)\n",
    "    np.random.shuffle(inputList);\n",
    "    length=len(inputList)\n",
    "    batchNum=int(np.ceil(length/size))\n",
    "    newList=[]\n",
    "    for i in range(batchNum-2):\n",
    "        newList.append(inputList[i*size:(i+1)*size])\n",
    "    newList.append(inputList[(batchNum-1)*size:])\n",
    "    return np.array(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(5, 5),\n",
    "                input_shape=(new_height, new_width, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "1000/1000 [==============================] - 6s 6ms/step\n",
      "Iteration 0 Validation Score: [0.4729894995689392, 0.7919999957084656]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 1 Validation Score: [0.4489898681640625, 0.8544999957084656]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 2 Validation Score: [0.20763647556304932, 0.9284999966621399]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 3 Validation Score: [0.12367212772369385, 0.9710000157356262]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 4 Validation Score: [0.9586667418479919, 0.8504999876022339]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 5 Validation Score: [0.23552654683589935, 0.8805000185966492]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 6 Validation Score: [0.09116445481777191, 0.9555000066757202]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 7 Validation Score: [0.06025252863764763, 0.972000002861023]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 8 Validation Score: [0.12595343589782715, 0.9539999961853027]\n",
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Iteration 9 Validation Score: [0.19407203793525696, 0.9449999928474426]\n"
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "modelLib=[]\n",
    "sd=np.random.randint(0,1000)\n",
    "test_images=SGDShuffle(dataList,1000,sd)\n",
    "test_labels=SGDShuffle(dataTag,1000,sd)\n",
    "for j in range(epochCount):\n",
    "    sd=np.random.randint(0,1000)\n",
    "    mini_images=SGDShuffle(dataList,miniBatchSize,sd)\n",
    "    mini_labels=SGDShuffle(dataTag,miniBatchSize,sd)\n",
    "\n",
    "    for i in range(len(mini_labels)):\n",
    "        model.fit(mini_images[i], mini_labels[i], batch_size=len(mini_images[i]),\n",
    "                callbacks=[history],verbose=0)\n",
    "    index=np.random.randint(0,len(test_images))\n",
    "    score = model.evaluate(test_images[index], test_labels[index], batch_size=len(test_labels[index]))\n",
    "    modelLib.append(model)\n",
    "    print(\"Iteration\", j, \"Validation Score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model=modelLib[9]\n",
    "saveModel('modelD1S256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
