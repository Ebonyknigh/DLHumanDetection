{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from itertools import repeat\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "import keras.callbacks as cb\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "new_width=64\n",
    "new_height=128\n",
    "channel=3\n",
    "epochCount=10\n",
    "miniBatchSize=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(name):\n",
    "    model_json = model.to_json()\n",
    "    jsName=name+'.json'\n",
    "    h5Name=name+'.h5'\n",
    "    with open(jsName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(h5Name)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pos images from path specified at posList\n",
    "def loadPosImage(lst):\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        width, height = im.size   # Get dimensions\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "        im=im.crop((left, top, right, bottom))\n",
    "        im=np.asarray(im)\n",
    "        _,_,z=im.shape\n",
    "        if(z==4):\n",
    "            im=np.delete(im,3,axis=2)\n",
    "        #im=im.reshape(new_width*new_height*3)\n",
    "        result.append(im)\n",
    "    return result\n",
    "\n",
    "def loadNegImage(lst):\n",
    "    scaleRatio=1.5 #tuned with training dataset\n",
    "    result=[]\n",
    "    for entry in lst:\n",
    "        im=Image.open(entry, 'r')\n",
    "        baseWidth=im.size[0]\n",
    "        baseHeight=im.size[1]\n",
    "        im = im.resize((int(baseWidth/scaleRatio),int(baseHeight/scaleRatio)), Image.ANTIALIAS)\n",
    "        width, height = im.size   # Get dimensions\n",
    "        width=width-new_width\n",
    "        height=height-new_height\n",
    "        #select top left from available range\n",
    "        curWidth=0\n",
    "        curHeight=0\n",
    "        while(curWidth<width):\n",
    "            curHeight=0\n",
    "            while(curHeight<height):\n",
    "                #print(curWidth, curHeight)\n",
    "                top=curHeight\n",
    "                left=curWidth\n",
    "                img=im.crop((left, top, left+new_width, top+new_height))\n",
    "                img=np.asarray(img)\n",
    "                _,_,z=img.shape\n",
    "                if(z==4):\n",
    "                    img=np.delete(img,3,axis=2)\n",
    "                #im=im.reshape(new_width*new_height*3)\n",
    "                result.append(img)\n",
    "                #move height 128 pixel a time\n",
    "                curHeight+=128\n",
    "            curWidth+=64\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGDShuffle(inputList, size,sd):\n",
    "    np.random.seed(sd)\n",
    "    np.random.shuffle(inputList);\n",
    "    length=len(inputList)\n",
    "    batchNum=int(np.ceil(length/size))\n",
    "    newList=[]\n",
    "    for i in range(batchNum-2):\n",
    "        newList.append(inputList[i*size:(i+1)*size])\n",
    "    newList.append(inputList[(batchNum-1)*size:])\n",
    "    return np.array(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg images are stored at: ./INRIAPerson/train_64x128_H96/neg.lst\n",
      "pos images are stored at: ./INRIAPerson/train_64x128_H96/pos.lst\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"./INRIAPerson/train_64x128_H96\"\n",
    "neg=\"neg.lst\"\n",
    "pos='pos.lst'\n",
    "neg=os.path.join(train_dir, neg)\n",
    "pos=os.path.join(train_dir, pos)\n",
    "print(\"neg images are stored at:\",neg)\n",
    "print(\"pos images are stored at:\",pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two lists that contain locations of positive and negative images\n",
    "posList=[]\n",
    "negList=[]\n",
    "with open(neg, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        negList.append(line)\n",
    "with open(pos, 'r') as f:\n",
    "    for line in f:\n",
    "        line=line[6:].strip('\\n')\n",
    "        line=os.path.join(train_dir, line)\n",
    "        posList.append(line)\n",
    "#Repeat each item in negList 3 times, for image reuse\n",
    "#negList = [x for item in negList for x in repeat(item, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416  pos images,  8156 neg images\n"
     ]
    }
   ],
   "source": [
    "posImgList=loadPosImage(posList)\n",
    "negImgList=loadNegImage(negList)\n",
    "posLength=len(posImgList)\n",
    "negLength=len(negImgList)\n",
    "print(posLength, \" pos images, \", negLength, \"neg images\")\n",
    "#create tag for each image\n",
    "posTag=[[0,1]]*posLength\n",
    "negTag=[[1,0]]*negLength\n",
    "dataList=[]\n",
    "dataList.extend(posImgList)\n",
    "dataList.extend(negImgList)\n",
    "dataList=np.array(dataList)\n",
    "#Input feature Scaling\n",
    "dataList=np.array(dataList)/255\n",
    "#dataList=np.append(posImgList, negImgList, axis = 0)\n",
    "dataTag=[]\n",
    "dataTag.extend(posTag)\n",
    "dataTag.extend(negTag)\n",
    "dataTag=np.array(dataTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(5, 5),\n",
    "                input_shape=(new_height, new_width, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Conv2D(16, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(16))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingfengli/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "128/128 [==============================] - 1s 8ms/step\n",
      "Iteration 0 Validation Score: [0.21904505789279938, 0.8515625]\n",
      "128/128 [==============================] - 1s 6ms/step\n",
      "Iteration 1 Validation Score: [0.17207178473472595, 0.9140625]\n",
      "128/128 [==============================] - 1s 6ms/step\n",
      "Iteration 2 Validation Score: [0.05776204913854599, 0.97265625]\n",
      "128/128 [==============================] - 1s 6ms/step\n",
      "Iteration 3 Validation Score: [0.08661049604415894, 0.984375]\n",
      "128/128 [==============================] - 1s 6ms/step\n",
      "Iteration 4 Validation Score: [0.10197088122367859, 0.9765625]\n",
      "128/128 [==============================] - 1s 5ms/step\n",
      "Iteration 5 Validation Score: [0.10470319539308548, 0.94140625]\n",
      "128/128 [==============================] - 1s 6ms/step\n",
      "Iteration 6 Validation Score: [0.06155113875865936, 0.9765625]\n",
      "128/128 [==============================] - 1s 4ms/step\n",
      "Iteration 7 Validation Score: [0.00970312487334013, 1.0]\n",
      "128/128 [==============================] - 0s 3ms/step\n",
      "Iteration 8 Validation Score: [0.02052820660173893, 0.9921875]\n",
      "128/128 [==============================] - 0s 4ms/step\n",
      "Iteration 9 Validation Score: [0.01208300981670618, 1.0]\n",
      "128/128 [==============================] - 1s 4ms/step\n",
      "Iteration 10 Validation Score: [0.0874754935503006, 0.95703125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cd0a94d26513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         model.fit(mini_images[i], mini_labels[i], batch_size=len(mini_images[i]),\n\u001b[0;32m---> 13\u001b[0;31m                 callbacks=[history],verbose=0)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "modelLib=[]\n",
    "sd=np.random.randint(0,1000)\n",
    "test_images=SGDShuffle(dataList,miniBatchSize,sd)\n",
    "test_labels=SGDShuffle(dataTag,miniBatchSize,sd)\n",
    "for j in range(epochCount):\n",
    "    sd=np.random.randint(0,1000)\n",
    "    mini_images=SGDShuffle(dataList,miniBatchSize,sd)\n",
    "    mini_labels=SGDShuffle(dataTag,miniBatchSize,sd)\n",
    "\n",
    "    for i in range(len(mini_labels)):\n",
    "        model.fit(mini_images[i], mini_labels[i], batch_size=len(mini_images[i]),\n",
    "                callbacks=[history],verbose=0)\n",
    "    index=np.random.randint(0,len(test_images))\n",
    "    score = model.evaluate(test_images[index], test_labels[index], batch_size=len(test_labels[index]))\n",
    "    modelLib.append(model)\n",
    "    print(\"Iteration\", j, \"Validation Score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model=modelLib[9]\n",
    "saveModel('modelD2T9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
